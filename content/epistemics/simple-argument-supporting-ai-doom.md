---
title: Simple argument supporting AI doom
tags:
  - draft
---
I have thought of a relatively simple argument for why p(doom) (or at least p(doom) conditional on us deploying superintelligence) might be high. Here it is:
- The Kolmogorov complexity of our human values is HUGE. 
- Humanity wasn't even on track to cover all human values, in other words, our current optimization pressure to aligning AI is really low
- There will be a superintelligent optimizer in the future.
- Its optimization pressure is astronomically larger than ours.
- Thus, assuming we even control that AI, the value we feed to the AI, which has undergone low optimization pressure, will be subject to extreme Goodhart
- If you want to build a superintelligent optimizer, the only way to avoid this scenario is to somehow represent this absolutely complicated value for it to have, which requires efforts vastly beyond anything humanity has done so far

